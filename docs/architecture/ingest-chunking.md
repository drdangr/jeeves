# Стратегия чанкирования для документов базы знаний

## Основные принципы

При разработке стратегии чанкирования для структурированных документов, особенно небольшого размера с высокой плотностью терминов и связей, следует руководствоваться следующими принципами:

1. **Семантическая целостность** - чанки должны соответствовать логическим блокам контента
    
2. **Минимизация дублирования** - перекрытие должно быть осмысленным, не полным повторением
    
3. **Сохранение контекста** - ключевые определения и термины должны быть доступны
    
4. **Адаптивность** - параметры чанкирования зависят от структуры и размера документа
    

## Оптимизированные параметры чанкирования

|Категория документов Размер (символы) Стратегия|||
|---|---|---|
|Микро (до 800)|< 800|Не чанкировать, сохранять целиком|
|Малые (800-2000)|800-2000|2-3 чанка, перекрытие 20-25%|
|Средние (2000-5000)|2000-5000|Стандартное чанкирование, 600-800 символов/чанк|
|Большие (>5000)|> 5000|Полное чанкирование с учетом структуры|

## Улучшенный алгоритм

```
def smart_chunking(text, filename):
    """Интеллектуальное чанкирование с учетом размера и структуры документа"""
    # Анализ документа
    doc_stats = analyze_document(text)
    
    # Базовое решение по категории размера
    if doc_stats["length"] < 800:
        return [create_whole_document_chunk(text, filename)]
    elif doc_stats["length"] < 2000:
        return create_small_document_chunks(text, filename, doc_stats)
    else:
        return create_standard_chunks(text, filename, doc_stats)

```

### Специальная обработка малых документов (800-2000 символов)

```
def create_small_document_chunks(text, filename, stats):
    """Создание 2-3 чанков для малых документов с учетом структуры"""
    chunks = []
    
    # Определение типа документа
    if stats["list_items_count"] > 3:
        # Документ-список
        chunks = split_list_document(text, stats["list_items_count"])
    elif stats["paragraphs_count"] > 2:
        # Документ с абзацами
        chunks = split_by_paragraphs(text, target_chunks=2)
    else:
        # Простое разделение
        midpoint = find_sentence_boundary(text, len(text) // 2)
        chunks = [text[:midpoint], text[midpoint:]]
    
    # Выделение блока тегов
    tag_block = extract_tag_block(text)
    if tag_block and len(tag_block) > 50:
        # Добавляем блок тегов к последнему чанку
        chunks[-1] = chunks[-1] if tag_block in chunks[-1] else chunks[-1] + "\n" + tag_block
    
    # Обогащение метаданными
    return [enrich_chunk(chunk, filename, text, i) for i, chunk in enumerate(chunks)]

```

### Умное определение границ чанков

```
def find_sentence_boundary(text, target_position):
    """Находит ближайшую границу предложения к target_position"""
    # Ищем ближайшую точку или конец элемента списка
    sentence_end = text.find(". ", target_position)
    list_item_end = text.find("\n- ", target_position)
    paragraph_end = text.find("\n\n", target_position)
    
    # Определяем ближайшую естественную границу
    candidates = [p for p in [sentence_end, list_item_end, paragraph_end] if p > 0]
    if candidates:
        return min(candidates) + 1
    else:
        return target_position

```

### Специальная обработка блоков тегов

```
def extract_tag_block(text):
    """Извлекает блок тегов в конце документа"""
    # Шаблон блока: несколько #тегов в конце документа
    match = re.search(r'(#\w+\s*)+$', text.strip())
    if match:
        return match.group(0)
    return None

def handle_tag_block(chunks, tag_block):
    """Обеспечивает целостную обработку блока тегов"""
    # Если блок тегов выделился в отдельный маленький чанк, объединяем
    if any(chunk.strip() == tag_block for chunk in chunks):
        # Находим чанк с блоком тегов
        tag_chunk_idx = next(i for i, chunk in enumerate(chunks) 
                             if chunk.strip() == tag_block)
        # Присоединяем к предыдущему чанку
        if tag_chunk_idx > 0:
            chunks[tag_chunk_idx-1] += "\n" + chunks[tag_chunk_idx]
            chunks.pop(tag_chunk_idx)
    return chunks

```

### Улучшенное обогащение чанков метаданными

```
def enrich_chunk(chunk, filename, full_text, chunk_index):
    """Расширенное обогащение чанка метаданными"""
    
    # Базовые метаданные
    metadata = {
        "text": chunk,
        "source": filename,
        "chunk_index": chunk_index,
        "position": {"start": full_text.find(chunk), "end": full_text.find(chunk) + len(chunk)}
    }
    
    # Извлечение вики-ссылок
    wiki_links = re.findall(r'\[\[(.*?)\]\]', chunk)
    
    # Извлечение неразмеченных терминов (ключевые слова)
    unmarked_terms = extract_key_terms(chunk, wiki_links)
    
    # Извлечение тегов из чанка
    chunk_tags = re.findall(r'#(\w+)', chunk)
    
    # Если тегов нет в чанке, но они есть в документе - наследуем
    if not chunk_tags and "#" in full_text:
        doc_tags = re.findall(r'#(\w+)', full_text)
        chunk_tags = doc_tags
    
    # Обогащаем метаданные
    metadata["links"] = wiki_links
    metadata["terms"] = unmarked_terms
    metadata["tags"] = chunk_tags
    
    return metadata

```

### Извлечение неразмеченных ключевых терминов

```
def extract_key_terms(text, known_links):
    """Извлекает ключевые термины, не оформленные как вики-ссылки"""
    # Извлекаем термины, начинающиеся с заглавной буквы (потенциальные имена собственные)
    potential_terms = re.findall(r'\b([А-Я][а-я]+)\b', text)
    
    # Фильтруем по известным ключевым терминам из базы знаний
    # (этот список должен быть доступен заранее)
    key_terms_db = get_key_terms_database()
    
    # Исключаем термины, которые уже есть в вики-ссылках
    extracted_terms = [term for term in potential_terms 
                      if term in key_terms_db and not any(term in link for link in known_links)]
    
    return extracted_terms

```

## Оптимизация перекрытия чанков

Традиционное перекрытие часто приводит к полному дублированию информации между чанками. Улучшенный подход:

```
def create_smart_overlap(chunks, overlap_size=150):
    """Создает умное перекрытие между чанками"""
    result = [chunks[0]]
    
    for i in range(1, len(chunks)):
        current_chunk = chunks[i]
        previous_chunk = chunks[i-1]
        
        # Находим смысловую границу для перекрытия
        overlap_text = previous_chunk[-overlap_size:]
        
        # Если граница попадает на середину элемента списка, находим начало элемента
        if "- " in overlap_text:
            list_item_start = overlap_text.rfind("\n- ")
            if list_item_start >= 0:
                overlap_text = overlap_text[list_item_start:]
        
        # Добавляем минимально необходимое перекрытие
        if not current_chunk.startswith(overlap_text):
            current_chunk = overlap_text + current_chunk
        
        result.append(current_chunk)
    
    return result

```

## Конкретные улучшения на основе тестирования

При тестировании стратегии чанкирования на небольших структурированных документах были выявлены следующие улучшения:

1. **Определение категорий документов**:
    
    - Документы 800-2000 символов требуют специальной обработки (2-3 чанка)
        
    - Для документов-списков важно сохранять целостность элементов списка
        
2. **Обработка блоков тегов**:
    
    - Небольшие блоки тегов (< 150 символов) не должны выделяться в отдельные чанки
        
    - Теги документа должны наследоваться всеми его чанками
        
3. **Смысловые границы чанков**:
    
    - Использование естественных границ предложений
        
    - Сохранение целостности элементов списка и определений
        
    - Предотвращение создания слишком маленьких чанков (< 300 символов)
        
4. **Распознавание терминов**:
    
    - Идентификация ключевых терминов без вики-разметки
        
    - Связывание с известными концепциями базы знаний
        

## Применение в рабочем процессе

Для структурированных документов (1500-2000 символов):

1. Определить тип структуры (список, определение, описание)
    
2. Разделить на 2-3 логических блока по естественным границам
    
3. Обработать блок тегов как часть последнего чанка
    
4. Обогатить чанки метаданными, включая ссылки и теги
    
5. Добавить умное перекрытие между чанками без полного дублирования
    

Это обеспечит оптимальный баланс между размером чанков, релевантностью поиска и эффективностью хранения.

---

## Перспективы развития

- **Динамическое чанкирование через эмбеддинги**: При необходимости более глубокой семантической обработки текстов (особенно для менее структурированных глав романа) может применяться построение эмбеддингов предложений и определение смысловых границ на основе изменений эмбеддингов.
    
- **Гибкость параметров чанкирования**: Параметры длины чанков, степени перекрытия и тип разбиения будут вынесены в отдельную конфигурацию, что позволит адаптировать стратегию под разные типы документов без необходимости модифицировать код.
    
- **Динамическое перекрытие чанков**: Перекрытие между чанками будет рассчитываться пропорционально размеру чанка: меньшее перекрытие для малых чанков, большее — для крупных.
    
- **Объединение коротких чанков**: Чанки размером менее 300 символов будут автоматически объединяться с соседними для поддержания логической целостности и плотности информации.
    
- **Расширенное определение смысловых границ**: При определении границ чанков будет учитываться не только окончание предложений ("."), но также разделители вроде тире ("—"), двоеточий (":"), точек с запятой (";") для более точного вычленения смысловых блоков. При необходимости более глубокой семантической обработки текстов (особенно для менее структурированных глав романа) может применяться построение эмбеддингов предложений и определение смысловых границ на основе изменений эмбеддингов.
    
- **Гибкость параметров чанкирования**: Параметры длины чанков, степени перекрытия и тип разбиения будут вынесены в отдельную конфигурацию, что позволит адаптировать стратегию под разные типы документов без необходимости модифицировать код.