# Стратегия чанкирования для документов базы знаний

## Содержание

Введение
Категоризация документов
Алгоритм выбора стратегии
Методы чанкирования

1. Чанкирование микро документов
2. Чанкирование малых документов
3. Чанкирование средних документов
4. Чанкирование больших документов
Унифицированная тематическая обработка


Базовые подфункции

Простое разбиение на чанки
Продвинутое разбиение на чанки
Обработка блоков тегов
Семантическое объединение чанков
Обогащение метаданными
Извлечение неразмеченных ключевых терминов


Тематическая кластеризация чанков

Концепция тематического графа
Процесс интеграции с чанкированием
Преимущества иерархического подхода
Интеграция с поиском


Примеры трансформации документов
Тестирование и оценка качества
Перспективы развития

## Введение

Разработка стратегии чанкирования направлена на обеспечение семантической целостности данных, удобства поиска и эффективного хранения информации. Важнейшими принципами являются:

- Сохранение логических блоков.
    
- Минимизация дублирования информации.
    
- Поддержание контекста ключевых терминов и тегов.
    
- Адаптивность параметров в зависимости от структуры и размера документа.
    
- **Тематическая кластеризация** — интеграция чанков в иерархическую модель знаний, где темы образуют верхний уровень графа, а чанки - нижний, с гибкой мультитематической принадлежностью.
    
- **Единый словарь терминов** — формирование и поддержание централизованного реестра всех терминов, их связей, упоминаний и тематической принадлежности.
    

> **Примечание**: Формирование иерархического тематического графа происходит в многопроходном процессе инжестирования. Первоначальный граф создается до этапа чанкирования, затем чанки интегрируются с весами принадлежности к темам, и на последнем этапе граф обновляется с учетом новой информации.

> **Примечание**: Единый словарь терминов позволяет не только улучшить поиск, но и выявить "непривязанные" термины (упоминаемые без соответствующих статей), что используется модулем Learner для приоритизации создания новых материалов.

# Тематическая кластеризация чанков

## Концепция тематического графа и единого словаря терминов

В основе стратегии чанкирования и последующей работы с базой знаний лежат две ключевые структуры данных:

1. **Иерархический тематический граф** — двухуровневая структура, где:
    
    - **Верхний уровень** — темы и их взаимосвязи:
        - Узлы: тематические области (термины, понятия, категории)
        - Ребра: семантические связи между темами с весами и типами отношений
    - **Нижний уровень** — гибкая привязка чанков к темам:
        - Каждый чанк может относиться к множеству тем с разными весами принадлежности
        - Нет жесткой иерархии, чанк не "принадлежит" только одной теме
2. **Единый словарь терминов** — централизованный реестр всех терминов базы знаний:
    
    - **Ключи** — нормализованные формы терминов (например, "внешники")
    - **Значения** — структурированная информация о термине (статьи, теги, темы, упоминания)

Эти структуры взаимно дополняют друг друга:

- Словарь терминов помогает нормализовать и объединить разные формы упоминания одного и того же понятия
- Тематический граф обеспечивает семантическую связность между терминами и контекстом их использования

### Структура иерархического тематического графа

```json
{
  "themes": [
    {
      "id": "аватара",
      "definition_doc": "Аватара.md",
      "related_themes": [
        {"theme_id": "воплощение", "weight": 0.85, "relation_type": "uses"},
        {"theme_id": "операторы", "weight": 0.72, "relation_type": "used_by"}
      ]
    },
    {
      "id": "операторы",
      "definition_doc": "Операторы.md",
      "related_themes": [
        {"theme_id": "аватара", "weight": 0.72, "relation_type": "uses"},
        {"theme_id": "гроссмейстеры", "weight": 0.90, "relation_type": "includes"}
      ]
    }
  ],
  "chunks": [
    {
      "id": "chunk_1",
      "text": "Аватара - невероятно точная...",
      "source": "Аватара.md",
      "theme_weights": {
        "аватара": 0.95,
        "операторы": 0.65,
        "воплощение": 0.58
      }
    }
  ]
}
```

> **Примечание**: Полное описание структуры тематического графа, включая методы расчета весов между темами, типы семантических отношений и алгоритмы обхода графа, будет представлено в [[thematic-graph3]].

### Структура словаря терминов

```json
{
  "внешники": {
    "название": "Внешники",
    "ссылки_на_статьи": ["Внешники.md", "Внешний мир и конфликт.md"],
    "теги": ["внешники", "внешние_территории"],
    "темы": ["внешний_мир", "конфликт", "социум"],
    "упоминания_без_ссылок": ["Лабиринт.md", "Зелень.md"]
  }
}
```

### Стратегические преимущества единого словаря терминов

1. **Выявление "пробелов" в базе знаний**:
    
    - Термины с большим количеством упоминаний, но без соответствующих статей
    - Автоматическое формирование задач для модуля Learner по созданию новых статей
2. **Нормализация терминологии**:
    
    - Предотвращение дублирования из-за разного написания одних и тех же понятий
    - Унификация представления терминов во всей базе знаний
3. **Семантическое обогащение поиска**:
    
    - Расширение запросов связанными терминами и тегами
    - Повышение полноты результатов поиска
4. **Мониторинг качества базы знаний**:
    
    - Отслеживание непокрытых областей
    - Выявление противоречий в использовании терминов

## Процесс интеграции с чанкированием

Интеграция иерархического тематического графа и словаря терминов с процессом чанкирования происходит в несколько этапов:

1. **Предварительное построение структур** (выполняется до чанкирования):
    
    - **Верхний уровень графа**:
        - Анализ всех документов для выявления потенциальных тематических областей
        - Идентификация определяющих документов для каждой темы
        - Формирование графа связей между темами с весами и типами отношений
    - **Словарь терминов**:
        - Извлечение и нормализация терминов из всех документов
        - Определение связей между терминами и документами
        - Выявление синонимов и альтернативных написаний
2. **Тематическая разметка при чанкировании** (выполняется во время чанкирования):
    
    - Каждый чанк анализируется на предмет принадлежности к разным темам
    - Рассчитываются веса принадлежности к темам на основе:
        - Тегов, крослинков, явных упоминаний
        - Семантической близости к определяющим документам тем
        - Наличия терминов из единого словаря
    - Чанк обогащается метаданными о тематических весах
    - Единый словарь терминов пополняется новой информацией
3. **Интеграция чанков в нижний уровень тематического графа** (выполняется после чанкирования):
    
    - Каждый чанк связывается с соответствующими темами верхнего уровня
    - Веса связей отражают степень принадлежности чанка к каждой теме
    - При наличии сильной принадлежности чанка к нескольким темам (>0.7), эти темы получают дополнительное усиление связи между собой
4. **Пост-обработка структур** (выполняется после завершения всех этапов):
    
    - Оптимизация графа для эффективного поиска и навигации
    - Выявление ключевых тематических кластеров
    - Формирование отчета о "непокрытых" терминах для модуля Learner

## Преимущества иерархического тематического графа

1. **Улучшенный поиск**:
    
    - Двухуровневая стратегия поиска: сначала среди тем, затем среди чанков
    - Возможность обхода графа на уровне тем для нахождения связанных концепций
    - Повышение точности при работе с многозначными терминами за счет контекста темы
2. **Контекстное обогащение**:
    
    - Автоматическое добавление связанных по теме чанков для обеспечения полноты контекста
    - Выявление косвенных связей через промежуточные темы
    - Более глубокое понимание семантической структуры базы знаний
3. **Навигация по базе знаний**:
    
    - Интуитивное перемещение по семантической карте знаний
    - Визуализация тематических кластеров и их взаимосвязей
    - Выявление ключевых концептов и "мостов" между разными областями знаний
4. **Структурирование информации**:
    
    - Выявление естественной тематической структуры базы знаний
    - Определение "белых пятен" — областей, требующих дополнительного контента
    - Формирование логически связанных групп документов и чанков
5. **Техническая эффективность**:
    
    - Сокращение пространства поиска за счет предварительной фильтрации по темам
    - Возможность кэширования отношений между темами верхнего уровня
    - Параллельная обработка запросов внутри отдельных тематических групп

## Пример структуры метаданных чанка с тематической информацией

```json
{
  "text": "Аватара - невероятно точная и чуткая антропоморфная машина, способная полностью имитировать действия тела воплощенного в него Оператора...",
  "source": "Аватара.md",
  "chunk_index": 0,
  "position": { "start": 0, "end": 351 },
  "links": ["Операторы", "Воплощение"],
  "terms": ["Аватара"],
  "tags": ["операторы", "эссенция", "аватара", "воплощение"],
  "themes": {
    "аватара": 0.95,
    "операторы": 0.85,
    "воплощение": 0.72,
    "эссенция": 0.31
  },
  "term_dictionary_links": {
    "Аватара": {
      "dictionary_key": "аватара",
      "type": "unmarked",
      "definition_exists": true
    },
    "Операторы": {
      "dictionary_key": "операторы",
      "type": "wikilink",
      "definition_exists": true
    }
  }
}
```

## Интеграция с поиском

Иерархический тематический граф позволяет реализовать многоуровневый поиск, который работает сначала на уровне тем, а затем на уровне чанков:

```python
def hierarchical_thematic_search(query):
    """Поиск с использованием иерархического тематического графа и словаря терминов"""
    # 1. Нормализация терминов в запросе с помощью словаря
    query_terms = extract_potential_terms(query)
    normalized_terms = normalize_query_terms(query_terms)
    
    # 2. Расширение запроса связанными терминами
    expanded_terms = expand_terms_using_dictionary(normalized_terms)
    
    # 3. Определяем релевантные темы верхнего уровня
    relevant_themes = identify_relevant_themes(query, expanded_terms)
    
    # 4. Если найдены релевантные темы, используем двухуровневый поиск
    if relevant_themes:
        # Этап 1: Обход графа на уровне тем
        # Находим дополнительные темы, связанные с исходными
        all_relevant_themes = find_connected_themes(relevant_themes)
        
        # Этап 2: Переход на нижний уровень (чанки)
        # Собираем чанки из всех релевантных тем с учетом весов
        weighted_chunks = []
        for theme, theme_relevance in all_relevant_themes.items():
            # Получаем чанки, связанные с этой темой
            theme_chunks = get_chunks_by_theme(theme)
            
            for chunk in theme_chunks:
                # Учитываем:
                # 1. Вес связи чанка с темой
                # 2. Релевантность темы запросу
                chunk_weight = chunk['themes'][theme] * theme_relevance
                
                # Добавляем чанк с его весом от этой темы
                if any(c['id'] == chunk['id'] for c in weighted_chunks):
                    # Если чанк уже добавлен (от другой темы), обновляем вес
                    for c in weighted_chunks:
                        if c['id'] == chunk['id'] and c['weight'] < chunk_weight:
                            c['weight'] = chunk_weight
                else:
                    # Иначе добавляем новый чанк
                    weighted_chunks.append({
                        'id': chunk['id'],
                        'text': chunk['text'],
                        'weight': chunk_weight
                    })
        
        # Этап 3: Финальный векторный поиск среди отфильтрованных чанков
        # с применением предварительных весов
        return vector_search_with_weights(query, weighted_chunks)
    
    # 5. Если тематические области не выявлены, используем стандартный поиск
    return standard_vector_search(query)
```

Этот подход обеспечивает:

1. Работу с верхним уровнем тем для предварительной фильтрации
2. Учет сложных семантических связей между темами
3. Более точное ранжирование результатов с учетом мультитематичности чанков
4. Возможность кэширования отношений между темами для ускорения поиска

> **Важно**: Данный раздел описывает только интеграцию тематического графа и словаря терминов с процессом чанкирования. Детальное описание создания и обновления самих структур будет представлено в документе по инжестированию.

---

## Категоризация документов

|Категория|Размер (символы)|Стратегия|
|---|---|---|
|Микро|< 800|Не чанкировать|
|Малые|800–2000|2–3 чанка, мягкое перекрытие|
|Средние|2000–5000|Стандартное чанкирование|
|Большие|> 5000|Стандартное чанкирование + постобработка на эмбеддингах|

### Обоснование выбора параметров

Эмпирически установлено, что оптимальные размеры чанков должны соответствовать определенным когнитивным и техническим характеристикам:

- **Порог 800 символов** (~120-150 слов) — минимальный размер для смыслового блока, содержащего достаточный контекст. Документы меньшего размера не нуждаются в разбиении, так как обычно представляют собой единое целое (определение термина, короткая заметка).
    
- **Порог 2000 символов** (~300-400 слов) — размер, при котором текст начинает содержать несколько смысловых блоков, но еще достаточно компактен для простого алгоритма разбиения.
    
- **Порог 5000 символов** (~800-1000 слов) — граница, после которой текст уже имеет сложную структуру с многоуровневыми логическими связями, требующими семантического анализа для сохранения смысла при чанкировании.
    
- **Размер перекрытия 100 символов** выбран как оптимальный для сохранения контекста между чанками — достаточный для включения нескольких терминов, но не избыточный для минимизации дублирования.
    

Приведенные параметры были оптимизированы на основе анализа корпуса документов базы знаний "Мир Странника" и могут быть адаптированы при необходимости через конфигурационные переменные.

![Диаграмма процесса чанкирования](https://i.imgur.com/placeholder_chunking_diagram.png)

---

# Алгоритм выбора стратегии

```python
def smart_chunking(text, filename):
    doc_stats = analyze_document(text)

    if doc_stats['length'] < 800:
        return create_whole_document_chunk(text, filename)
    elif doc_stats['length'] < 2000:
        return create_small_document_chunks(text, filename, doc_stats)
    elif doc_stats['length'] <= 5000:
        return create_standard_document_chunks(text, filename, doc_stats)
    else:
        return create_big_document_chunks(text, filename, doc_stats)
```

---

# Методы чанкирования

## 1. Чанкирование микро документов

```python
def create_whole_document_chunk(text, filename):
    return [enrich_chunk(text, filename, text, 0)]
```

- Весь документ становится одним чанком.
    
- Обогащение метаданными.
    

---

## 2. Чанкирование малых документов (800–2000 символов)

```python
def create_small_document_chunks(text, filename, stats):
    chunks = simple_chunk_split(text)
    chunks = handle_tag_block(chunks, text)
    return [enrich_chunk(chunk, filename, text, i) for i, chunk in enumerate(chunks)]
```

- Разделение на 2–3 чанка.
    
- Мягкое перекрытие.
    
- Обработка блоков тегов.
    
- Обогащение метаданными.
    

---

## 3. Чанкирование средних документов (2000–5000 символов)

```python
def create_standard_document_chunks(text, filename, stats):
    chunks = advanced_chunk_split(text)
    chunks = handle_tag_block(chunks, text)
    return [enrich_chunk(chunk, filename, text, i) for i, chunk in enumerate(chunks)]
```

- Продвинутое разбиение по структуре и смысловым блокам.
    
- Учет абзацев, терминов, маркдауна.
    
- Обогащение чанков метаданными.
    

---

## 4. Чанкирование больших документов (>5000 символов)

```python
def create_big_document_chunks(text, filename, stats):
    chunks = advanced_chunk_split(text)
    chunks = semantic_chunking_postprocess(chunks)
    chunks = handle_tag_block(chunks, text)
    return [enrich_chunk(chunk, filename, text, i) for i, chunk in enumerate(chunks)]
```

- Стандартное продвинутое разбиение.
    
- Постобработка на эмбеддингах для объединения логически связанных чанков.
    
- Обогащение метаданными.
    

## Унифицированная тематическая обработка для всех категорий документов

Независимо от выбранного метода чанкирования (для микро, малых, средних или больших документов), все чанки проходят одинаковую процедуру тематической классификации через функцию `enrich_chunk`. Это гарантирует, что каждый чанк получит свои тематические веса и будет правильно интегрирован в иерархический тематический граф.

Такой унифицированный подход обеспечивает:

1. **Тематическую целостность базы знаний** — все чанки, независимо от размера исходного документа, становятся частью единого иерархического тематического пространства
    
2. **Консистентность поиска** — иерархический поиск работает одинаково эффективно для всех категорий документов
    
3. **Масштабируемость** — при добавлении новых тем, подтем или типов отношений не требуется переписывать алгоритмы чанкирования
    

### Особенности интеграции чанков в иерархический граф

При интеграции чанков в иерархический граф соблюдаются следующие принципы:

1. **Гибкая мультитематическая принадлежность**:
    
    - Чанк может относиться к любому количеству тем
    - Каждая связь чанк-тема имеет свой вес, отражающий силу принадлежности
2. **Обратное влияние**:
    
    - Чанки с сильной принадлежностью к нескольким темам усиливают связи между этими темами
    - Этот механизм обратной связи обеспечивает улучшение верхнего уровня графа на основе данных чанкирования
3. **Интеграционный процесс**:
    
    - На первом этапе определяются веса принадлежности чанка к существующим темам
    - На втором этапе чанк добавляется в нижний уровень графа
    - На третьем этапе происходит обновление весов связей между темами

> **Примечание**: В случае микро-документов, которые не разбиваются на чанки, тематическая обработка происходит для всего документа как единого чанка, что позволяет маленьким, но важным определениям быть корректно представленными в иерархическом тематическом графе.

---

# Базовые подфункции

## Простое разбиение на чанки

```python
def simple_chunk_split(text):
    """Простое разбиение текста на чанки по абзацам и предложениям с мягким перекрытием."""
    import re

    # Этап 1: Делим текст по двойным переносам строк (абзацы)
    paragraphs = re.split(r'\n\s*\n', text.strip())

    # Этап 2: Склеиваем абзацы в чанки целевого размера
    chunks = []
    current_chunk = ""
    target_size = 800  # Целевой размер чанка
    overlap_size = 100  # Перекрытие между чанками

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        # Если текущий чанк + новый абзац не превышают целевой размер
        if len(current_chunk) + len(para) + 2 <= target_size:
            current_chunk += "\n\n" + para if current_chunk else para
        else:
            # Завершаем текущий чанк
            if current_chunk:
                chunks.append(current_chunk.strip())
            # Стартуем новый
            current_chunk = para

    # Добавляем последний чанк, если он не пуст
    if current_chunk:
        chunks.append(current_chunk.strip())

    # Этап 3: Добавляем перекрытие между чанками
    overlapped_chunks = []
    for i, chunk in enumerate(chunks):
        if i > 0:
            # Добавляем перекрытие из конца предыдущего чанка
            overlap = chunks[i-1][-overlap_size:].strip()
            chunk = overlap + "\n\n" + chunk
        overlapped_chunks.append(chunk.strip())

    return overlapped_chunks
```

## Продвинутое разбиение на чанки

```python
def advanced_chunk_split(text):
    """Продвинутое разбиение текста на чанки с учётом структуры, маркдауна и терминов."""
    import re

    # Этап 1: Разделяем текст по двойным переносам строк (основные абзацы)
    paragraphs = re.split(r'\n\s*\n', text.strip())

    # Этап 2: Учитываем специальные структуры Markdown
    refined_paragraphs = []
    buffer = ""

    for para in paragraphs:
        # Определяем, является ли параграф специальным элементом Markdown
        is_special = para.startswith(('-', '*', '+', '>')) or re.match(r'^#{1,6}\s', para)
        is_code_block = para.strip().startswith('```') or para.strip().endswith('```')
        has_wikilinks = '[[' in para and ']]' in para  # Проверка на наличие вики-ссылок
        
        # Определяем, содержит ли параграф известные термины
        contains_important_terms = False
        terms_dictionary = get_terms_dictionary()
        
        # Проверка на наличие важных терминов из словаря
        for term_key in terms_dictionary:
            term_name = terms_dictionary[term_key]["название"]
            if re.search(r'\b' + re.escape(term_name) + r'\b', para, re.IGNORECASE):
                contains_important_terms = True
                break
        
        # Параграфы со специальной разметкой, вики-ссылками или важными терминами
        # стараемся не разбивать между чанками
        if is_special or is_code_block or has_wikilinks or contains_important_terms:
            # Если это важный элемент, завершаем текущий буфер и добавляем элемент отдельно
            if buffer:
                refined_paragraphs.append(buffer.strip())
                buffer = ""
                
            # Специальные элементы добавляем отдельно, чтобы они не разрывались
            refined_paragraphs.append(para.strip())
        else:
            # Иначе накапливаем абзацы в буфер
            if buffer:
                buffer += "\n\n" + para
            else:
                buffer = para

    # Не забываем добавить последний буфер
    if buffer:
        refined_paragraphs.append(buffer.strip())

    # Этап 3: Склеиваем параграфы в чанки определённого размера,
    # с учетом минимизации разрывов важных контекстов
    chunks = []
    current_chunk = ""
    target_size = 800  # Прицеливаемся на размер чанка ~800 символов
    max_size = 1200    # Максимальный размер при наличии важных элементов
    
    for para in refined_paragraphs:
        current_size = len(current_chunk) + (2 if current_chunk else 0) + len(para)
        
        # Проверяем, содержит ли параграф вики-ссылки или важные термины
        has_important_content = '[[' in para or any(
            re.search(r'\b' + re.escape(terms_dictionary[key]["название"]) + r'\b', 
                     para, re.IGNORECASE) 
            for key in terms_dictionary
        )
        
        # Используем разные стратегии в зависимости от важности контента
        if has_important_content:
            # Для важного содержимого допускаем превышение обычного целевого размера
            if current_size <= max_size:
                current_chunk += "\n\n" + para if current_chunk else para
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para
        else:
            # Стандартная логика для обычного контента
            if current_size <= target_size:
                current_chunk += "\n\n" + para if current_chunk else para
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para

    if current_chunk:
        chunks.append(current_chunk.strip())

    # Этап 4: Добавляем перекрытие между чанками, обеспечивая 
    # сохранение контекста важных терминов
    overlapped_chunks = []
    for i, chunk in enumerate(chunks):
        if i > 0:
            last_chunk = chunks[i-1]
            overlap_size = 100  # Базовый размер перекрытия
            
            # Увеличиваем перекрытие если обнаружены вики-ссылки вблизи границы
            if '[[' in last_chunk[-200:]:
                # Найдем последнюю вики-ссылку в конце предыдущего чанка
                last_link_start = last_chunk.rfind('[[')
                if last_link_start != -1 and last_link_start > len(last_chunk) - 200:
                    last_link_end = last_chunk.find(']]', last_link_start)
                    if last_link_end != -1:
                        # Увеличиваем перекрытие, чтобы включить всю ссылку и контекст
                        overlap_size = max(overlap_size, len(last_chunk) - last_link_start + 50)
            
            # Умное перекрытие: стараемся захватить целые предложения
            overlap_start = max(0, len(last_chunk) - overlap_size)
            
            # Находим конец последнего полного предложения в перекрытии
            sentence_end = last_chunk.rfind('. ', overlap_start)
            if sentence_end != -1 and sentence_end > overlap_start:
                overlap_start = sentence_end + 1
                
            overlap = last_chunk[overlap_start:].strip()
            chunk = overlap + "\n\n" + chunk
            
        overlapped_chunks.append(chunk.strip())

    return overlapped_chunks
```

## Обработка блоков тегов

```python
def handle_tag_block(chunks, text):
    """Обеспечивает целостную обработку блока тегов в конце документа."""
    import re

    # Пытаемся найти блок тегов в конце текста
    tag_block_match = re.search(r'(?:#\w+\s*)+$', text.strip())
    if not tag_block_match:
        return chunks

    tag_block = tag_block_match.group(0)

    # Ищем, содержится ли блок тегов отдельно в каком-то чанке
    for idx, chunk in enumerate(chunks):
        if chunk.strip() == tag_block.strip():
            # Присоединяем блок тегов к предыдущему чанку
            if idx > 0:
                chunks[idx-1] += '\n' + chunks[idx]
                del chunks[idx]
            break

    return chunks

```

## Семантическое объединение чанков

```python
def semantic_chunking_postprocess(chunks):
    """
    Обрабатывает список чанков, объединяя семантически близкие по смыслу.
    
    Данная функция анализирует эмбеддинги последовательных чанков и объединяет их,
    если они достаточно близки по смыслу. Это помогает:
    1. Уменьшить общее количество чанков
    2. Сохранить семантически связанные части в одном чанке
    3. Улучшить качество поиска для сложных запросов
    
    Args:
        chunks: список словарей с ключом 'text', содержащим текст чанка
        
    Returns:
        список объединенных чанков в том же формате
    """
    # Константы для настройки
    THRESHOLD = 0.75  # Порог косинусного сходства для объединения (0-1)
    MAX_CHUNK_SIZE = 1500  # Максимальный размер объединенного чанка
    
    # Генерируем эмбеддинги для всех чанков
    from sentence_transformers import SentenceTransformer
    embedding_model = SentenceTransformer('ai-forever/sbert_large_mt_nlu_ru')
    embeddings = embedding_model.encode([chunk['text'] for chunk in chunks])
    
    # Функция вычисления косинусного сходства
    def cosine_similarity(vec1, vec2):
        import numpy as np
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        return dot_product / (norm1 * norm2)
    
    # Объединяем чанки на основе семантического сходства
    merged_chunks = []
    buffer = chunks[0]['text']
    buffer_embedding = embeddings[0]
    
    for i in range(1, len(chunks)):
        similarity = cosine_similarity(buffer_embedding, embeddings[i])
        
        # Объединяем, если смысловое сходство высокое и не превышаем максимальный размер
        if similarity > THRESHOLD and len(buffer) + len(chunks[i]['text']) <= MAX_CHUNK_SIZE:
            # Объединяем текст с правильным форматированием
            buffer += '\n\n' + chunks[i]['text']
            
            # Пересчитываем эмбеддинг объединенного текста
            # Это важно для последующих сравнений
            buffer_embedding = embedding_model.encode([buffer])[0]
        else:
            # Сохраняем накопленный буфер и начинаем новый
            merged_chunks.append(buffer)
            buffer = chunks[i]['text']
            buffer_embedding = embeddings[i]
    
    # Добавляем последний обработанный буфер
    if buffer:
        merged_chunks.append(buffer)
    
    # Преобразуем результаты обратно в формат словарей
    return [{'text': chunk} for chunk in merged_chunks]
```

### Визуализация процесса семантического объединения

Рассмотрим, как работает процесс объединения чанков на примере:

```
Чанк 1: "Описание технологии Воплощения и её принципов работы..."
Чанк 2: "Принципы работы технологии продолжение. Примеры использования..."
Чанк 3: "История открытия Эссенции и её влияние на людей..."
```

**Шаг 1:** Вычисляем эмбеддинги для каждого чанка.

**Шаг 2:** Сравниваем Чанк 1 и Чанк 2:

- Косинусное сходство = 0.82 (> 0.75)
- Объединяем их в новый чанк: "Описание технологии Воплощения и её принципов работы... Принципы работы технологии продолжение. Примеры использования..."

**Шаг 3:** Сравниваем объединенный чанк и Чанк 3:

- Косинусное сходство = 0.65 (< 0.75)
- Оставляем их отдельно

**Результат:**

```
Объединенный чанк 1: "Описание технологии Воплощения... Примеры использования..."
Чанк 2: "История открытия Эссенции и её влияние на людей..."
```

Таким образом, близкие по смыслу фрагменты объединяются, а разные темы остаются разделенными, даже если они были в одном документе.

## Обогащение метаданными

```python
def enrich_chunk(chunk, filename, full_text, chunk_index):
    """Расширенное обогащение чанка метаданными"""
    import re

    # Базовые метаданные
    metadata = {
        "text": chunk,
        "source": filename,
        "chunk_index": chunk_index,
        "position": {
            "start": full_text.find(chunk),
            "end": full_text.find(chunk) + len(chunk)
        }
    }

    # Извлечение вики-ссылок
    wiki_links = re.findall(r'\[\[(.*?)\]\]', chunk)

    # Извлечение неразмеченных терминов (ключевые слова)
    unmarked_terms = extract_key_terms(chunk, wiki_links)

    # Извлечение тегов из чанка
    chunk_tags = re.findall(r'#(\w+)', chunk)

    # Если тегов нет в чанке, но они есть в документе — наследуем
    if not chunk_tags and "#" in full_text:
        doc_tags = re.findall(r'#(\w+)', full_text)
        chunk_tags = doc_tags

    # Обогащаем метаданные
    metadata["links"] = wiki_links
    metadata["terms"] = unmarked_terms
    metadata["tags"] = chunk_tags

    return metadata

```

## Извлечение неразмеченных ключевых терминов

```python
def extract_key_terms(text, known_links, current_file):
    """
    Извлекает ключевые термины, не оформленные как вики-ссылки,
    и обновляет единый словарь терминов
    """
    import re
    
    # Получаем доступ к единому словарю терминов
    terms_dictionary = get_terms_dictionary()
    
    # Извлекаем потенциальные термины: слова с заглавной буквы
    potential_terms = re.findall(r'\b([А-Я][а-я]+)\b', text)
    extracted_terms = []
    
    for term in potential_terms:
        # Нормализуем термин (единый регистр, удаление лишних символов)
        normalized = normalize_term(term)
        
        # Проверяем, что термин не является частью вики-ссылки
        if not any(normalized in link.lower() for link in known_links):
            # Если термин уже есть в словаре
            if normalized in terms_dictionary:
                # Если файл еще не в списке упоминаний без ссылок
                if current_file not in terms_dictionary[normalized]["упоминания_без_ссылок"]:
                    terms_dictionary[normalized]["упоминания_без_ссылок"].append(current_file)
                extracted_terms.append(term)
            else:
                # Проверяем, является ли это потенциальным новым термином
                if is_potential_term(normalized):
                    # Создаем новую запись в словаре
                    terms_dictionary[normalized] = {
                        "название": term,
                        "ссылки_на_статьи": [],
                        "теги": find_matching_tags(normalized, terms_dictionary),
                        "темы": [],
                        "упоминания_без_ссылок": [current_file]
                    }
                    extracted_terms.append(term)
    
    # Сохраняем обновленный словарь
    save_terms_dictionary(terms_dictionary)
    
    return extracted_terms

def normalize_term(term):
    """Нормализует термин для единообразного представления в словаре"""
    # Приводим к нижнему регистру
    normalized = term.lower()
    # Удаляем лишние пробелы и спецсимволы
    normalized = re.sub(r'\s+', ' ', normalized).strip()
    # Можно добавить другие правила нормализации
    return normalized

def is_potential_term(normalized):
    """
    Определяет, является ли слово потенциальным термином,
    используя эвристики и внешние словари
    """
    # Минимальная длина для термина
    if len(normalized) < 3:
        return False
    
    # Можно добавить другие эвристики:
    # - Проверка по словарю общих слов (чтобы исключить обычные существительные)
    # - Проверка на основе частотности в корпусе (необычные слова с большей вероятностью термины)
    # - Семантический анализ с использованием внешних моделей
    
    return True

def find_matching_tags(normalized, terms_dictionary):
    """Находит теги, которые могут соответствовать термину"""
    matching_tags = []
    
    # Ищем прямые совпадения в тегах
    all_tags = get_all_document_tags()
    for tag in all_tags:
        if normalized == tag.lower() or normalized in tag.lower():
            matching_tags.append(tag)
    
    # Можно добавить поиск по связанным терминам и их тегам
    for term, data in terms_dictionary.items():
        if term != normalized and (term in normalized or normalized in term):
            matching_tags.extend(data.get("теги", []))
    
    return list(set(matching_tags))  # Убираем дубликаты
```

### Единый словарь терминов

Единый словарь терминов — это централизованный реестр всех значимых понятий базы знаний, содержащий информацию об их взаимосвязях, употреблении и тематической принадлежности. Его структура выглядит следующим образом:

```json
{
  "внешники": {
    "название": "Внешники",
    "ссылки_на_статьи": ["Внешники.md", "Внешний мир и конфликт.md"],
    "теги": ["внешники", "внешние_территории"],
    "темы": ["внешний_мир", "конфликт", "социум"],
    "упоминания_без_ссылок": ["Лабиринт.md", "Зелень.md"]
  },
  "аватара": {
    "название": "Аватара",
    "ссылки_на_статьи": ["Аватара.md"],
    "теги": ["аватара", "воплощение"],
    "темы": ["технологии", "операторы"],
    "упоминания_без_ссылок": []
  }
}
```

#### Основные элементы структуры:

- **название** — каноническая форма термина с правильным регистром
- **ссылки_на_статьи** — список статей, непосредственно описывающих данный термин
- **теги** — связанные хэштеги, обнаруженные в документах
- **темы** — тематические области, к которым относится термин
- **упоминания_без_ссылок** — список статей, где термин упоминается без оформления в виде вики-ссылки

#### Преимущества этого подхода:

1. **Нормализация терминов** — предотвращение дублирования из-за разного написания
2. **Выявление "пробелов" в базе знаний** — термины, часто упоминаемые без соответствующих статей
3. **Расширение поисковых запросов** — через связанные термины и теги
4. **Приоритизация для модуля Learner** — создание новых статей для непокрытых терминов

Словарь терминов формируется в процессе инжестирования и постоянно обновляется при добавлении новых документов и чанков.

---

# Примеры трансформации документов

Чтобы наглядно продемонстрировать работу алгоритмов чанкирования, рассмотрим несколько конкретных примеров.

## Пример 1: Микро-документ (<800 символов)

**Исходный документ:**

```markdown
# Аватара

Аватара - невероятно точная и чуткая антропоморфная машина, способная полностью имитировать действия тела воплощенного в него [[Операторы|Оператора]]. 
Аватара практически полностью (в зависимости от модели) дублирует человеческую  нервную систему и благодаря технологии [[Воплощение|Воплощения]] получает на нее сигналы от нервной системы [[Операторы|Оператора]].

#операторы #эссенция #аватара #воплощение #аватара
```

**Результат чанкирования:** Документ не разбивается (единый чанк), но обогащается метаданными:

```json
{
  "text": "# Аватара\n\nАватара - невероятно точная и чуткая антропоморфная машина...",
  "source": "Аватара.md",
  "chunk_index": 0,
  "position": { "start": 0, "end": 351 },
  "links": ["Операторы", "Воплощение", "Операторы"],
  "terms": [],
  "tags": ["операторы", "эссенция", "аватара", "воплощение", "аватара"]
}
```

## Пример 2: Средний документ (~3000 символов)

**Исходный документ: "Воплощение.md"** (фрагмент начала)

```markdown
**Воплощение - эксклюзивная технология Лабиринт**, благодаря которой осуществляется подключение [Оператора] к устройствам, которые управляются его нервной системой. Для считывания сигналов нервной системы [[Операторы|Оператора]] и передачи данных обратно необходима [Эссенция]

В основе Воплощения лежит сильно прогрессировавшая технология нейроинтерфейсов...
[остальной текст документа]
```

**Результат чанкирования:** Документ разбивается на 4 чанка с перекрытием:

**Чанк 1:**

```markdown
**Воплощение - эксклюзивная технология Лабиринт**... [первые ~800 символов]

Все жители [Ультраполисов] в той или иной степени способны управлять техникой...
```

**Чанк 2:**

```markdown
Все жители [Ультраполисов] в той или иной степени способны управлять техникой... [перекрытие с Чанком 1]

Но только те их них, кто способны к высокой степени концентрации...
```

Каждый чанк обогащается метаданными, включая ссылки, теги и позиции в исходном документе.

## Пример 3: Обработка блока тегов

Если блок тегов оказывается в отдельном чанке, алгоритм объединяет его с предыдущим чанком:

**До обработки:**

```
Чанк N-1: "...последние абзацы документа."
Чанк N: "#тег1 #тег2 #тег3"
```

**После обработки:**

```
Чанк N-1: "...последние абзацы документа.\n\n#тег1 #тег2 #тег3"
```

# Тестирование и оценка качества

## Метрики оценки качества чанкирования

1. **Семантическая целостность** - насколько хорошо сохраняется смысловая целостность чанков. Оценивается через:
    
    - Косинусное сходство между эмбеддингами чанка и соответствующей части исходного документа
    - Экспертная оценка сохранения смысла
2. **Эффективность поиска** - оценивается с помощью:
    
    - Precision@k - точность возвращения релевантных чанков
    - Recall@k - полнота возвращения релевантных чанков
    - Mean Reciprocal Rank (MRR) - среднее обратного ранга первого релевантного результата
3. **Техническая эффективность**:
    
    - Равномерность распределения размеров чанков
    - Минимизация дублирования информации
    - Скорость обработки

## Процесс тестирования

1. **Тестовый набор данных**:
    
    - Выборка документов различных категорий (микро, малые, средние, большие)
    - Набор запросов с эталонными ответами
2. **Автоматическое тестирование**:
    
    - Запуск чанкирования на тестовом наборе
    - Сравнение результатов поиска с эталонными ответами
    - Расчет метрик качества
3. **Диагностика и отладка**:
    
    - Визуализация разбивки документов
    - Идентификация проблемных случаев
    - Анализ ложных срабатываний и пропусков

## Перспективы развития

- **Динамическое управление длиной чанков** по плотности информации:
    
    - Адаптивное определение размера чанка на основе семантической плотности текста
    - Использование информационной энтропии для более точного разбиения
- **Расширенная иерархическая модель тематического графа**:
    
    - Введение дополнительных уровней иерархии (мета-темы, подтемы)
    - Автоматическое выявление тематической иерархии на основе семантической близости
    - Применение алгоритмов графового машинного обучения для оптимизации связей
    - Внедрение типизированных отношений между темами (включает, противоположно, зависит)
- **Расширение обработки событийной разметки (TimeLine)**:
    
    - Обнаружение и индексирование временных маркеров
    - Связывание событий в цепочки с сохранением причинно-следственных связей
    - Интеграция временной оси как дополнительного измерения в тематический граф
- **Параллельная обработка для ускорения работы на больших базах**:
    
    - Распараллеливание обработки документов между потоками
    - Сегментирование иерархического графа для распределенных вычислений
    - Оптимизация памяти для работы с очень крупными документами
- **Интеграция с LLM для улучшения качества чанкирования**:
    
    - Использование языковых моделей для определения границ смысловых блоков
    - Генерация сжатых метаданных для каждого чанка
    - Автоматическое определение наиболее подходящих типов отношений между темами
    - Интеллектуальное выявление тематической принадлежности с помощью LLM
- **Улучшенная визуализация и навигация**:
    
    - Интерактивная карта знаний с иерархическим представлением тем и подтем
    - Тепловые карты тематической насыщенности документов и чанков
    - Персонализированные рекомендации на основе истории изучения тематических областей
    - Адаптивная агрегация графа в зависимости от уровня масштабирования

---

## Интеграция с архитектурой Дживса

Модуль чанкирования интегрируется с остальными компонентами системы:

1. **Взаимодействие с [[docs/architecture/ingest|Ingest]]**:
    
    - Принимает исходные документы и их метаданные
    - Возвращает готовые чанки для индексации
2. **Поддержка [[docs/architecture/seeker|Seeker]]**:
    
    - Оптимизирует чанки для эффективного поиска
    - Включает метаданные для фильтрации и ранжирования
3. **Обратная связь от [[docs/architecture/analyst|Analyst]]**:
    
    - Анализ качества ответов для улучшения стратегии чанкирования
    - Адаптация параметров под конкретные типы запросов

---